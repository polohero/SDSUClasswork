{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS596 Machine Learning \n",
    "# Homework Assignment 4 (Part 1): Shallow Neural Network\n",
    "\n",
    "### Due 11:59 pm, Friday, 10/12/2018\n",
    "\n",
    "**Total credits: 6.5**\n",
    "\n",
    "In Part 1 of HA4, we will implement a 2-layer shallow neural network model. \n",
    "\n",
    "We will use the model to conduct the same binary classification task as HA3, i.e., classify two categories of the sign language dataset. \n",
    "\n",
    "The input size is the number of pixels in a image ($64\\times 64$). The size of hidden layer is determined by a hyperparameter `n_h`, and the size of output layer is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "# import importlib\n",
    "# importlib.reload(utils)\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4096, 286)\n",
      "(1, 286)\n",
      "(4096, 125)\n",
      "(1, 125)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "X_train, Y_train, X_test, Y_test = load_data()\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Intialize parameters\n",
    "**0.5 credit**\n",
    "\n",
    "The parameters associated with the hidden layer are $W^{[1]}$ and $b^{[1]}$, and the parameters associated with the output layer are $W^{[2]}$ and $b^{[2]}$.\n",
    "\n",
    "We use **tanh** as acitivation function for hidden layer, and **sigmoid** for output layer.\n",
    "\n",
    "**Instructions:**\n",
    "- Initialize parameters randomly\n",
    "- Use `np.random.randn((size_out, size_in))*0.01` to initialize $W^{[l]}$, in which `size_out` is the output size of current layer, and `size_in` is the input size from previous layer. \n",
    "- Use `np.zeros()` to initialize $b^{[l]}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(n_i, n_h, n_o):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    n_i -- size of input layer\n",
    "    n_h -- size of hidden layer\n",
    "    n_o -- size of output layer\n",
    "    \n",
    "    Return:\n",
    "    params -- a dict object containing all parameters:\n",
    "        W1 -- weight matrix of layer 1\n",
    "        b1 -- bias vector of layer 1\n",
    "        W2 -- weight matrix of layer 2\n",
    "        b2 -- bias vector of layer 2\n",
    "    \"\"\"\n",
    "    np.random.seed(2) # DO NOT change this line! \n",
    "    \n",
    "    ### START TODO ###\n",
    "    W1 = np.random.randn(n_h, n_i)*0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_o, n_h)*0.01\n",
    "    b2 = np.zeros(n_o)\n",
    "    ### END TODO ###\n",
    "    \n",
    "    params = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-0.00416758 -0.00056267 -0.02136196]\n",
      " [ 0.01640271 -0.01793436 -0.00841747]\n",
      " [ 0.00502881 -0.01245288 -0.01057952]\n",
      " [-0.00909008  0.00551454  0.02292208]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[ 0.00041539 -0.01117925  0.00539058 -0.0059616 ]]\n",
      "b2 = [0.]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Task\n",
    "ps = init_params(3, 4, 1)\n",
    "print('W1 =', ps['W1'])\n",
    "print('b1 =' ,ps['b1'])\n",
    "print('W2 =', ps['W2'])\n",
    "print('b2 =', ps['b2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "\n",
    "|&nbsp;|&nbsp; |          \n",
    "|--|--|\n",
    "|**W1 =**|[[-0.00416758 -0.00056267 -0.02136196] <br>[ 0.01640271 -0.01793436 -0.00841747] <br> [ 0.00502881 -0.01245288 -0.01057952]<br>[-0.00909008  0.00551454  0.02292208]]|\n",
    "|**b1 =**|[[0.]<br>[0.]<br>[0.]<br>[0.]]|\n",
    "|**W2 =**|[[ 0.00041539 -0.01117925  0.00539058 -0.0059616 ]]|\n",
    "|**b2 =**|[[0.]]|\n",
    "\n",
    "***\n",
    "\n",
    "### 1.2 Forward propagation\n",
    "\n",
    "**1.5 credit**\n",
    "\n",
    "Use the following fomulas to implement forward propagation:\n",
    "- $Z^{[1]} = W^{[1]}X + b^{[1]}$\n",
    "- $A^{[1]} = tanh(Z^{[1]})$ --> use `np.tanh` function\n",
    "- $Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]}$\n",
    "- $A^{[2]} = \\sigma(Z^{[2]})$ --> directly use the `sigmoid` function provided in `utils` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X, params):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    X -- input data of shape (n_in, m)\n",
    "    params -- a python dict object containing all parameters (output of init_params)\n",
    "    \n",
    "    Return:\n",
    "    A2 -- the activation of the output layer\n",
    "    cache -- a python dict containing all intermediate values for later use in backprop\n",
    "             i.e., 'Z1', 'A1', 'Z2', 'A2'\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # Retrieve parameters\n",
    "    ### START TODO ###\n",
    "    W1 = params['W1']\n",
    "    b1 = params['b1']\n",
    "    W2 = params['W2']\n",
    "    b2 = params['b2']\n",
    "    ### END TODO ###\n",
    "    \n",
    "    # Implement forward propagation\n",
    "    ### START TODO ###\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    ### END TODO ###\n",
    "    \n",
    "    assert A1.shape[1] == m\n",
    "    assert A2.shape[1] == m\n",
    "    \n",
    "    cache = {'Z1': Z1, 'A1': A1, 'Z2': Z2, 'A2': A2}\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean(Z1) = 0.1320898134744307\n",
      "mean(A1) = -0.012947502242343013\n",
      "mean(Z2) = -0.028697749001905516\n",
      "mean(A2) = 0.5329353691451202\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Task\n",
    "X_tmp, params_tmp = forwardprop_testcase()\n",
    "\n",
    "A2, cache = forward_prop(X_tmp, params_tmp)\n",
    "print('mean(Z1) =', np.mean(cache['Z1']))\n",
    "print('mean(A1) =', np.mean(cache['A1']))\n",
    "print('mean(Z2) =', np.mean(cache['Z2']))\n",
    "print('mean(A2) =', np.mean(cache['A2']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "\n",
    "|&nbsp;|&nbsp; |          \n",
    "|--|--|\n",
    "|**mean(Z1) =**|0.13208981347443063|\n",
    "|**mean(A1) =**|-0.01294750224234301|\n",
    "|**mean(Z2) =**|-0.028697749001905536|\n",
    "|**mean(A2) =**|0.5329353691451202|\n",
    "\n",
    "***\n",
    "\n",
    "### 1.3 Backward propagation\n",
    "**2 credit**\n",
    "\n",
    "Use the following formulas to implement backward propagation:\n",
    "- $dZ^{[2]} = A^{[2]} - Y$\n",
    "- $dW^{[2]} = \\frac{1}{m}dZ^{[2]}A^{[1]T}$ --> $m$ is the number of examples\n",
    "- $db^{[2]} = \\frac{1}{m}$ np.sum( $dZ^{[2]}$, axis=1, keepdims=True)\n",
    "- $dA^{[1]} = W^{[2]T}dZ^{[2]}$\n",
    "- $dZ^{[1]} = dA^{[1]}*g'(Z^{[1]})$\n",
    "    - Here $*$ denotes element-wise multiply\n",
    "    - $g(z)$ is the tanh function, therefore its derivative $g'(Z^{[1]}) = 1 - (g(Z^{[1]}))^2 = 1 - (A^{[1]})^2$\n",
    "- $dW^{[1]} = \\frac{1}{m} dZ^{[1]}X^T$\n",
    "- $db^{[1]} = \\frac{1}{m}$ np.sum( $dZ^{[1]}$, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_prop(X, Y, params, cache):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    X -- input data of shape (n_in, m)\n",
    "    Y -- input label of shape (1, m)\n",
    "    params -- a python dict containing all parameters\n",
    "    cache -- a python dict containing 'Z1', 'A1', 'Z2' and 'A2' (output of forward_prop)\n",
    "    \n",
    "    Return:\n",
    "    grads -- a python dict containing the gradients w.r.t. all parameters,\n",
    "             i.e., dW1, db1, dW2, db2\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # Retrieve parameters\n",
    "    ### START TODO ###\n",
    "    W1 = params['W1']\n",
    "    W2 = params['W2']\n",
    "    ### END TODO ###\n",
    "    \n",
    "    # Retrive intermediate values stored in cache\n",
    "    ### START TODO ###\n",
    "    A1 = cache['A1']\n",
    "    A2 = cache['A2']\n",
    "    ### END TODO ###\n",
    "    \n",
    "    # Implement backprop\n",
    "    ### START TODO ###\n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = (1 / m) * np.dot(dZ2, A1.T)\n",
    "    db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.square(A1))\n",
    "    dW1 = (1 / m) * np.dot(dZ1, X.T)\n",
    "    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    ### END TODO ###\n",
    "    \n",
    "    grads = {'dW1': dW1, 'db1': db1, 'dW2': dW2, 'db2': db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean(dW1) -0.039558211695590706\n",
      "mean(db1) 0.001467912358907287\n",
      "mean(dW2) 0.1250823230639841\n",
      "mean(db2) 0.13293536800000003\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Task\n",
    "X_tmp, Y_tmp, params_tmp, cache_tmp = backprop_testcase()\n",
    "\n",
    "grads = backward_prop(X_tmp, Y_tmp, params_tmp, cache_tmp)\n",
    "print('mean(dW1)', np.mean(grads['dW1']))\n",
    "print('mean(db1)', np.mean(grads['db1']))\n",
    "print('mean(dW2)', np.mean(grads['dW2']))\n",
    "print('mean(db2)', np.mean(grads['db2']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "\n",
    "|&nbsp;|&nbsp; |          \n",
    "|--|--|\n",
    "|**mean(dW1) =**|-0.039558211695590706|\n",
    "|**mean(db1) =**|0.001467912358907287|\n",
    "|**mean(dW2) =**|0.1250823230639841|\n",
    "|**mean(db2) =**|0.13293536800000003|\n",
    "\n",
    "***\n",
    "\n",
    "### 1.4 Update parameters\n",
    "**0.5 credit**\n",
    "\n",
    "Update $W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}$ accordingly:\n",
    "- $W^{[1]} = W^{[1]} - \\alpha\\ dW^{[1]}$\n",
    "- $b^{[1]} = b^{[1]} - \\alpha\\ db^{[1]}$\n",
    "- $W^{[2]} = W^{[2]} - \\alpha\\ dW^{[2]}$\n",
    "- $b^{[2]} = b^{[2]} - \\alpha\\ db^{[2]}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(params, grads, alpha):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    params -- a python dict containing all parameters\n",
    "    grads -- a python dict containing the gradients w.r.t. all parameters (output of backward_prop)\n",
    "    alpha -- learning rate\n",
    "    \n",
    "    Return:\n",
    "    params -- a python dict containing all updated parameters\n",
    "    \"\"\"\n",
    "    # Retrieve parameters\n",
    "    ### START TODO ###\n",
    "    W1 = params['W1']\n",
    "    b1 = params['b1']\n",
    "    W2 = params['W2']\n",
    "    b2 = params['b2']\n",
    "    ### END TODO ###\n",
    "    \n",
    "    # Retrieve gradients\n",
    "    ### START TODO ###\n",
    "    dW1 = grads['dW1']\n",
    "    db1 = grads['db1']\n",
    "    dW2 = grads['dW2']\n",
    "    db2 = grads['db2']\n",
    "    ### END TODO ###\n",
    "    \n",
    "    # Update each parameter\n",
    "    \n",
    "    ### START TODO ###\n",
    "    W1 = W1 - alpha * dW1\n",
    "    b1 = b1 - alpha * db1\n",
    "    W2 = W2 - alpha * dW2\n",
    "    b2 = b2 - alpha * db2\n",
    "    ### END TODO ###\n",
    "    \n",
    "    params = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-1.9959083  -1.06667372 -0.75475925]\n",
      " [ 0.29418098 -0.98950432 -1.22186039]\n",
      " [-0.27355221 -1.6082775   0.11104952]\n",
      " [-0.14229044  1.07321512  0.4208789 ]\n",
      " [ 0.59648267  0.86048013 -1.60750032]]\n",
      "b1 = [[-0.00062513]\n",
      " [ 0.00046574]\n",
      " [ 0.00069734]\n",
      " [-0.0002741 ]\n",
      " [-0.00033725]]\n",
      "W2 = [[ 1.08049444 -0.25269532  1.0989616   0.20063139  1.45914531]]\n",
      "b2 = [[-0.00132935]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Task\n",
    "params_tmp, grads_tmp = update_params_testcase()\n",
    "\n",
    "params = update_params(params_tmp, grads_tmp, 0.01)\n",
    "print('W1 =', params['W1'])\n",
    "print('b1 =', params['b1'])\n",
    "print('W2 =', params['W2'])\n",
    "print('b2 =', params['b2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "\n",
    "|&nbsp;|&nbsp; |          \n",
    "|--|--|\n",
    "|**W1 =**|[[-1.9959083  -1.06667372 -0.75475925]<br>[ 0.29418098 -0.98950432 -1.22186039]<br>[-0.27355221 -1.6082775   0.11104952]<br>[-0.14229044  1.07321512  0.4208789 ]<br>[ 0.59648267  0.86048013 -1.60750032]]|\n",
    "|**b1 =**|[[-0.00062513]<br>[ 0.00046574]<br>[ 0.00069734]<br>[-0.0002741 ]<br>[-0.00033725]]|\n",
    "|**W2 =**|[[ 1.08049444 -0.25269532  1.0989616   0.20063139  1.45914531]]|\n",
    "|**b2 =**|[[-0.00132935]]|\n",
    "\n",
    "***\n",
    "\n",
    "### 1.5 Integrated model\n",
    "**1 credit**\n",
    "\n",
    "Integrate `init_params`, `forward_prop`, `backward_prop` and `update_params` into one model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(X, Y, n_h, num_iters=10000, alpha=0.01, verbose=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    X -- training data of shape (n_in, m)\n",
    "    Y -- training label of shape (1, m)\n",
    "    n_h -- size of hidden layer\n",
    "    num_iters -- number of iterations for gradient descent\n",
    "    verbose -- print cost every 1000 steps\n",
    "    \n",
    "    Return:\n",
    "    params -- parameters learned by the model. Use these to make predictions on new data\n",
    "    \"\"\"\n",
    "    np.random.seed(3)\n",
    "    m = X.shape[1]\n",
    "    n_in = X.shape[0]\n",
    "    n_out = 1\n",
    "    \n",
    "    # Initialize parameters and retrieve them\n",
    "    params = init_params(n_in, n_h, n_out)\n",
    "    W1 = params['W1']\n",
    "    b1 = params['b1']\n",
    "    W2 = params['W2']\n",
    "    b2 = params['b2']\n",
    "    \n",
    "    # Gradient descent loop\n",
    "    for i in range(num_iters):\n",
    "        ### START TODO ###\n",
    "        A2, cache = forward_prop(X, params)\n",
    "        grads = backward_prop(X, Y, params, cache)\n",
    "        params = update_params(params, grads, alpha)\n",
    "        ### END TODO\n",
    "        \n",
    "        # Compute cost\n",
    "        cost = -(1/m) * np.sum(np.log(A2) * Y + np.log(1 - A2) * (1 - Y))\n",
    "        \n",
    "        # Print cost\n",
    "        if i % 1000 == 0 and verbose:\n",
    "            print('Cost after iter {}: {}'.format(i, cost))\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.728558   -0.60417473 -0.24274211]\n",
      " [ 0.88560809 -0.67439594 -0.3043778 ]\n",
      " [-0.20781606 -0.59195986 -0.27344463]\n",
      " [-0.51914662  0.61152697  0.17713134]\n",
      " [ 0.00864946 -0.25198231 -0.1411225 ]]\n",
      "b1 = [[ 0.29073376]\n",
      " [ 0.29189656]\n",
      " [ 0.28876041]\n",
      " [-0.32656432]\n",
      " [ 0.09711243]]\n",
      "W2 = [[-1.25312586 -1.40689892 -0.69967068  1.13815825 -0.31472553]]\n",
      "b2 = [[-0.80345148]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Task 1.5\n",
    "X_tmp, Y_tmp = nn_model_testcase()\n",
    "\n",
    "params_tmp = nn_model(X_tmp, Y_tmp, n_h=5, num_iters=5000, alpha=0.01)\n",
    "print('W1 =', params_tmp['W1'])\n",
    "print('b1 =', params_tmp['b1'])\n",
    "print('W2 =', params_tmp['W2'])\n",
    "print('b2 =', params_tmp['b2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "\n",
    "|&nbsp;|&nbsp; |          \n",
    "|--|--|\n",
    "|**W1 =**|[[ 0.728558   -0.60417473 -0.24274211]<br>[ 0.88560809 -0.67439594 -0.3043778 ]<br>[-0.20781606 -0.59195986 -0.27344463]<br>[-0.51914662  0.61152697  0.17713134]<br>[ 0.00864946 -0.25198231 -0.1411225 ]]|\n",
    "|**b1 =**|[[ 0.29073376]<br>[ 0.29189656]<br>[ 0.28876041]<br>[-0.32656432]<br>[ 0.09711243]]|\n",
    "|**W2 =**|[[-1.25312586 -1.40689892 -0.69967068  1.13815825 -0.31472553]]|\n",
    "|**b2 =**|[[-0.80345148]]|\n",
    "\n",
    "***\n",
    "\n",
    "### 1.6 Predict\n",
    "**0.5 credit**\n",
    "\n",
    "Use the learned parameters to make predictions on new data. \n",
    "- Compute $A^{[2]}$ by calling `forward_prop`. Note that the `cache` returned will not be used in making predictions.\n",
    "- Convert $A^{[2]}$ into a vector of 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, params):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    X -- input data of shape (n_in, m)\n",
    "    params -- a python dict containing the learned parameters\n",
    "    \n",
    "    Return:\n",
    "    pred -- predictions of model on X, a vector of 0s and 1s\n",
    "    \"\"\"\n",
    "    ### START TODO ###\n",
    "    A2, _ = forward_prop(X, params)\n",
    "    pred = np.copy(A2)\n",
    "    pred[pred >= .5] = 1 \n",
    "    pred[pred < .5] = 0\n",
    "    ### END TODO ###\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions =  [[0. 1. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Task 1.6\n",
    "# NOTE: the X_tmp and params_tmp are the ones generated in evaluating Task 1.5 (two cells above)\n",
    "pred = predict(X_tmp, params_tmp)\n",
    "print('predictions = ', pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "\n",
    "|&nbsp;|&nbsp; |          \n",
    "|--|--|\n",
    "|**predictions =**|[[0. 1. 0. 0. 1.]]|\n",
    "\n",
    "***\n",
    "\n",
    "### 1.7 Train and evaluate\n",
    "\n",
    "**0.5 credit**\n",
    "\n",
    "Train the neural network model on X_train and Y_train, and evaluate it on X_test and Y_test.\n",
    "\n",
    "You can use the code from HA3 to compute the accuracy of your predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iter 0: 0.6931077265775999\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-73942269c3ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Train the model on X_train and Y_train, and print cost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# DO NOT change the hyperparameters, so that your output matches the expected one.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_h\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Make predictions on X_test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-839375914836>\u001b[0m in \u001b[0;36mnn_model\u001b[1;34m(X, Y, n_h, num_iters, alpha, verbose)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_iters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;31m### START TODO ###\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mA2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_prop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackward_prop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-a9f8b4fbafbe>\u001b[0m in \u001b[0;36mforward_prop\u001b[1;34m(X, params)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m# Implement forward propagation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;31m### START TODO ###\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mZ1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m     \u001b[0mA1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZ1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mZ2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model on X_train and Y_train, and print cost\n",
    "# DO NOT change the hyperparameters, so that your output matches the expected one.\n",
    "params = nn_model(X_train, Y_train, n_h = 10, num_iters=10000, verbose=True)\n",
    "\n",
    "# Make predictions on X_test\n",
    "predictions = predict(X_test, params)\n",
    "\n",
    "# Compute accuracy\n",
    "### START TODO ###\n",
    "metrics = calc_metrics(Y_test, predictions)\n",
    "acc = metrics['Accuracy']\n",
    "### END TODO ###\n",
    "print('Accuracy = {0:.2f}%'.format(acc * 100))\n",
    "\n",
    "\n",
    "# Calculate TP, FP, TN, FN, Accuracy, Precision, Recall, and F-1 score\n",
    "# We assume that label y = 1 is positive, and y = 0 is negative\n",
    "def calc_metrics(Y_test, Y_pred_test):\n",
    "    \"\"\"\n",
    "    Calculate metrics\n",
    "    \n",
    "    Args:\n",
    "    Y_test -- test label\n",
    "    Y_pred_test -- predictions on test data\n",
    "    \n",
    "    Return:\n",
    "    metrics -- a dict object\n",
    "    \"\"\"\n",
    "    assert(Y_test.shape == Y_pred_test.shape)\n",
    "    \n",
    "    ##### START TODO #####\n",
    "    \n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "    \n",
    "    for i in range(len(Y_pred_test[0])): \n",
    "        TP += 1 if Y_pred_test[0, i] == Y_test[0,i] and Y_test[0,i] == 1 else 0\n",
    "        FP += 1 if Y_pred_test[0,i] == 1 and Y_test[0,i] == 0 else 0\n",
    "        TN += 1 if Y_pred_test[0,i] == 0 and Y_test[0,i] == 0 else 0\n",
    "        FN += 1 if Y_pred_test[0,i] == 0 and Y_test[0,i] == 1 else 0\n",
    "\n",
    "    Accuracy = ( TP + TN ) / ( TP + TN + FP + FN )\n",
    "    Precision = TP / ( TP + FP )\n",
    "    Recall = TP / ( TP + FN )\n",
    "    F1 = 2 * ( Precision * Recall ) / ( Precision + Recall )\n",
    "    \n",
    "    ##### END TODO #####\n",
    "    \n",
    "    metrics = {\n",
    "        'TP': TP,\n",
    "        'FP': FP,\n",
    "        'TN': TN,\n",
    "        'FN': FN,\n",
    "        'Accuracy': Accuracy,\n",
    "        'Precision': Precision,\n",
    "        'Recall': Recall,\n",
    "        'F1': F1\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "\n",
    "|&nbsp;|&nbsp; |          \n",
    "|--|--|\n",
    "|**Cost after iter 0:**|0.6931077265775999|\n",
    "|**Cost after iter 1000:**|0.2482306581297105|\n",
    "|**Cost after iter 2000:**|0.05471507033938196|\n",
    "|**Cost after iter 3000:**|0.024326463013581715|\n",
    "|**Cost after iter 4000:**|0.014595754197204438|\n",
    "|**Cost after iter 5000:**|0.010131520880123288|\n",
    "|**Cost after iter 6000:**|0.00764463387660483|\n",
    "|**Cost after iter 7000:**|0.0060842030981856435|\n",
    "|**Cost after iter 8000:**|0.005023835721723831|\n",
    "|**Cost after iter 9000:**|0.0042610856757679645|\n",
    "|**Accuracy =** |95.20%|\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
